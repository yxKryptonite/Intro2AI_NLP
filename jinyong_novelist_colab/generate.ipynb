{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "import sys\n",
    "THIS_PATH = '/content/drive/My Drive/jinyong_novelist_colab/'\n",
    "sys.path.append('/content/drive/My Drive/jinyong_novelist_colab/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from lgg_model import *\n",
    "from translate import *\n",
    "from dataload import *\n",
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vanilla_GRU(\n",
       "  (Embedding): Embedding(4191, 100)\n",
       "  (GRU): GRU(100, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (Linear): Linear(in_features=100, out_features=4191, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgg_model_path = input(\"请输入想使用的语言模型(无需添加后缀)：\")\n",
    "lgg_model_path = 'lgg_model_paths/' + lgg_model_path\n",
    "lgg_model = torch.load(lgg_model_path, map_location=device)\n",
    "lgg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = input(\"请输入想使用的文件名(无需添加后缀)：\")\n",
    "file_name = 'texts/' + file_name + '.txt'\n",
    "file = open(file_name, 'r', encoding='utf-8')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "wv, vocabulary_length = word_to_idx(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   郭靖本身拍来的衣衫，然有亏，自高是夏服机必不知归云庄去就先走。黄蓉拉着黄药师双手疾挥出去。黄蓉心肠秀，在流巅立即伸出手帕。朱聪向他易头，反手正要她从未伸出身去，假装大骇。\n",
      "\n",
      "\n",
      "\n",
      "　　彭连虎等见其数天明有“逍遥”下将遇到了敌人。欧阳锋大声感令猛害，正说为他折节，难以便是。”黄蓉道：“我在这姑娘身子轻功。”全金发再也辨人不易找去。\n",
      "\n",
      "\n",
      "\n",
      "　　郭靖走进房来，失声神色。\n",
      "\n",
      "\n",
      "\n",
      "　　包惜弱凛然说道：“师叔胡说。”杨康心道：“我磕头。”郭靖与郭靖拆不过激受重罪，已不离此而令，大家已要发……”郭靖与那少女说着道：“那么他说的所写的醉仙老太通，那真是甚么？不知我还没说过罢，和老顽童周师哥奔死小跑，当是这般红个究教。”\n",
      "\n",
      "\n",
      "\n",
      "　　欧阳锋在年长长重山，隔了一阵，戏弄傻姑的武功很是太性，当即往他上数里赏，相见武功的半数奥奇，不禁暗气骇异。那大汉道：“爹，我相见……”\n",
      "\n",
      "\n",
      "\n",
      "　　韩宝驹时时驱破黄药师的右手，骂道：“这个小王爷，你拉着窝阔台报仇，于是葬给法观战之人，蹲下身来。郭靖一齐再坐，只见花里一声吆喝，呼喊一声，左手伸出出来，一时倒退，如何用断，蛇蟠阵勇，那也就如法先招争地，倏间俨然是天和黑风双煞，重阳真人豁喇\n",
      "Generation finished.\n"
     ]
    }
   ],
   "source": [
    "words = input(\"请输入初始文本：\")\n",
    "del_lst = []\n",
    "lst = list(words)\n",
    "\n",
    "for i in lst:\n",
    "    if i not in wv.keys():\n",
    "        del_lst.append(i)\n",
    "for i in del_lst:\n",
    "    lst.remove(i)\n",
    "\n",
    "data = np.array([])\n",
    "for i in lst:\n",
    "    data = np.append(data, wv[i])\n",
    "\n",
    "count = int(input(\"请输入想要生成的字(词)数：\"))\n",
    "\n",
    "for i in lst:\n",
    "    print(i, end='')\n",
    "\n",
    "for i in range(count):\n",
    "    data = np.stack((data,))\n",
    "    x = torch.Tensor(data)\n",
    "    x = x.to(torch.long)\n",
    "    y = lgg_model(x)[0][-1]\n",
    "    p = y.detach().numpy()\n",
    "    p = softmax(p)\n",
    "\n",
    "    idx = np.random.choice(vocabulary_length, p=p)\n",
    "    for item in wv.items():\n",
    "        if item[1] == idx:\n",
    "            new_word = item[0]\n",
    "    print(new_word, end='')\n",
    "\n",
    "    lst.append(new_word)\n",
    "    data = np.append(data, idx)\n",
    "\n",
    "print('\\nGeneration finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'郭靖武艺高实，梁子翁素知要了人都另有其气，实是没患，再里把帐幕中的。\\n段天德万才有人爱吓。欧阳锋大喜，抿嘴笑道：“那真好喜无仇深。”黄蓉点头道：“怎么黄药师武功卓绝，碧应长路，四下乱悄擦到了帐口，见喜色惨白不得，无人不敢再过女是大汗。\\n四人伏在铁箱手里，顺手提着抢上岸去，神态暗暗为笑。黄蓉拉了一把小艇干粮。蓉儿在下身上周伯通，准以后意定始终应不起来，叫道：“你尽数做岳飞，赤老温跟你'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\n",
    "for i in lst:\n",
    "    string += str(i)\n",
    "string = string.replace('\\n\\n\\n\\n\\u3000\\u3000', '\\n')\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "靖武艺高实，梁子翁素知求之，皆有其气，不可。"
     ]
    }
   ],
   "source": [
    "Beg = 0\n",
    "End = 0\n",
    "\n",
    "for sub in string:\n",
    "    if sub == '\\n':\n",
    "        End = string.index(sub)\n",
    "        translated = inference(string[Beg:End])\n",
    "\n",
    "        for i in translated[0]:\n",
    "            if i == ' ':\n",
    "                continue\n",
    "            print(i, end='')\n",
    "        Beg = End + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "靖曰：不知此字物。不可。毒计必真当自杀，归戍未亥。不可。美图渐友遗属以城图。不可。不可。"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/generate.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/generate.ipynb#ch0000006?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m sub \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m，\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m sub \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m。\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m sub \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m！\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m sub \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m？\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m sub \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/generate.ipynb#ch0000006?line=10'>11</a>\u001b[0m     end \u001b[39m=\u001b[39m string\u001b[39m.\u001b[39mindex(sub)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/generate.ipynb#ch0000006?line=11'>12</a>\u001b[0m     translated \u001b[39m=\u001b[39m inference(string[beg:end])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/generate.ipynb#ch0000006?line=13'>14</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m translated[\u001b[39m0\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/generate.ipynb#ch0000006?line=14'>15</a>\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py:24\u001b[0m, in \u001b[0;36minference\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py?line=20'>21</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer([text,],\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtk_kwargs)\n\u001b[1;32m     <a href='file:///Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py?line=21'>22</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='file:///Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mbatch_decode(\n\u001b[0;32m---> <a href='file:///Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py?line=23'>24</a>\u001b[0m         model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='file:///Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py?line=24'>25</a>\u001b[0m         inputs\u001b[39m.\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='file:///Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py?line=25'>26</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49minputs\u001b[39m.\u001b[39;49mattention_mask,\n\u001b[1;32m     <a href='file:///Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py?line=26'>27</a>\u001b[0m         num_beams\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     <a href='file:///Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py?line=27'>28</a>\u001b[0m         bos_token_id\u001b[39m=\u001b[39;49m\u001b[39m101\u001b[39;49m,\n\u001b[1;32m     <a href='file:///Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py?line=28'>29</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49m\u001b[39m102\u001b[39;49m,\n\u001b[1;32m     <a href='file:///Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py?line=29'>30</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     <a href='file:///Users/kuangyuxuan/Desktop/22Spring/Intro2AI/NLP_/Intro2AI_NLP/jinyong_novelist_colab/translate.py?line=30'>31</a>\u001b[0m     ), skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py:1251\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1246'>1247</a>\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1247'>1248</a>\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1248'>1249</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1249'>1250</a>\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1250'>1251</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1251'>1252</a>\u001b[0m         input_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1252'>1253</a>\u001b[0m         beam_scorer,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1253'>1254</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1254'>1255</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1255'>1256</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1256'>1257</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1257'>1258</a>\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1258'>1259</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1259'>1260</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1260'>1261</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1261'>1262</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1263'>1264</a>\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1264'>1265</a>\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1265'>1266</a>\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1266'>1267</a>\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, typical_p\u001b[39m=\u001b[39mtypical_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=1267'>1268</a>\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py:2035\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=2030'>2031</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=2032'>2033</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=2034'>2035</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=2035'>2036</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=2036'>2037</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=2037'>2038</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=2038'>2039</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=2039'>2040</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=2041'>2042</a>\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py?line=2042'>2043</a>\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:514\u001b[0m, in \u001b[0;36mEncoderDecoderModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=508'>509</a>\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=509'>510</a>\u001b[0m         labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=510'>511</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=512'>513</a>\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=513'>514</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=514'>515</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=515'>516</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=516'>517</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=517'>518</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=518'>519</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=519'>520</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=520'>521</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=521'>522</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=522'>523</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=523'>524</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=524'>525</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_decoder,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=525'>526</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=527'>528</a>\u001b[0m \u001b[39m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=528'>529</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1047\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1038'>1039</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1039'>1040</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1040'>1041</a>\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1041'>1042</a>\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1042'>1043</a>\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1043'>1044</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1044'>1045</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1046'>1047</a>\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1047'>1048</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1048'>1049</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1049'>1050</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1050'>1051</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1051'>1052</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1052'>1053</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1053'>1054</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1054'>1055</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1055'>1056</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1056'>1057</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1057'>1058</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1058'>1059</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1059'>1060</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1060'>1061</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1061'>1062</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1063'>1064</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:890\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=879'>880</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=880'>881</a>\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=881'>882</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=886'>887</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=887'>888</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=888'>889</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=889'>890</a>\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=890'>891</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=891'>892</a>\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=892'>893</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=893'>894</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=894'>895</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=895'>896</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=896'>897</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=897'>898</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=898'>899</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=900'>901</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=901'>902</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:417\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=414'>415</a>\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=415'>416</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_cross_attn(hidden_states)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=416'>417</a>\u001b[0m cross_attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrossattention(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=417'>418</a>\u001b[0m     hidden_states,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=418'>419</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=419'>420</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=420'>421</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=421'>422</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=422'>423</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=423'>424</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=424'>425</a>\u001b[0m attn_output \u001b[39m=\u001b[39m cross_attn_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=425'>426</a>\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:336\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=333'>334</a>\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=334'>335</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=335'>336</a>\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=337'>338</a>\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=338'>339</a>\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:193\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=191'>192</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_attn\u001b[39m(\u001b[39mself\u001b[39m, query, key, value, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, head_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=192'>193</a>\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query, key\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=194'>195</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn_weights:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=195'>196</a>\u001b[0m         attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m/\u001b[39m (value\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# translate the generated text into classical Chinese\n",
    "string = \"\"\n",
    "for i in lst:\n",
    "    string += str(i)\n",
    "\n",
    "beg = 0\n",
    "end = 0\n",
    "\n",
    "for sub in string:\n",
    "    if sub == '，' or sub == '。' or sub == '！' or sub == '？' or sub == '\\n':\n",
    "        end = string.index(sub)\n",
    "        translated = inference(string[beg:end])\n",
    "\n",
    "        for i in translated[0]:\n",
    "            if i == ' ':\n",
    "                continue\n",
    "            print(i, end='')\n",
    "        beg = end + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['郭 靖 与 黄 蓉 至 二 人 ， 招 亲 于 众 时 驱 兵 编 帅 ，']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['周 成 队 者 亦 请 具 文 录 ， 谢 方 喘 息 ， 复 与 大 汉']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference('上周在课上组好队的同学也请填写一下文档吧，谢谢！正在喘息片刻，又和大汉说，我们放他们进来。')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
