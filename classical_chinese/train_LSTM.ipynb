{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "from encoder_decoder import *\n",
    "from dataload import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_classical = Word2Vec.load(\"word_model_paths/classical_wm_lunyu\").wv\n",
    "wv_modern = Word2Vec.load(\"word_model_paths/modern_wm_lunyu\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 64\n",
    "full_dataset = MyDataset_embed(wm_paths, data_path)\n",
    "\n",
    "train_in_all = 0.8 # train_in_all is the proportion of the dataset that will be used for training\n",
    "\n",
    "train_size = int(train_in_all * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-0.7334,  0.1840, -0.3414,  ..., -0.9254, -0.0097, -0.1916],\n",
       "          [-0.3039,  0.8708, -0.9427,  ..., -1.8476,  0.1600,  0.3162],\n",
       "          [-0.8228, -1.4249, -1.0220,  ..., -0.7598, -0.5913,  0.2913],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.8787, -2.2798,  0.0856,  ..., -0.4607,  0.7535,  0.3063],\n",
       "          [ 0.0638, -0.0541,  0.9432,  ..., -0.0452, -1.8661,  0.3621],\n",
       "          [ 2.0901, -1.1587, -0.5705,  ..., -0.8278, -0.6187,  1.0071],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.2556, -0.0378, -0.2397,  ..., -0.4543,  0.4115, -0.2153],\n",
       "          [-0.2912, -0.5154,  0.2299,  ..., -0.1213,  1.2323, -0.4701],\n",
       "          [-0.2381, -0.1770,  0.0093,  ..., -0.3520,  0.2868, -0.0523],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.5865, -0.4461, -0.1658,  ..., -0.1417, -1.3701, -0.9140],\n",
       "          [ 0.0638, -0.0541,  0.9432,  ..., -0.0452, -1.8661,  0.3621],\n",
       "          [ 2.0901, -1.1587, -0.5705,  ..., -0.8278, -0.6187,  1.0071],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.8787, -2.2798,  0.0856,  ..., -0.4607,  0.7535,  0.3063],\n",
       "          [ 0.0638, -0.0541,  0.9432,  ..., -0.0452, -1.8661,  0.3621],\n",
       "          [ 0.7598, -1.0955, -0.7968,  ...,  1.3863, -0.4902, -0.9520],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0638, -0.0541,  0.9432,  ..., -0.0452, -1.8661,  0.3621],\n",
       "          [ 0.8261,  1.5722,  0.1580,  ..., -0.3858, -1.8703,  0.2936],\n",
       "          [ 2.0901, -1.1587, -0.5705,  ..., -0.8278, -0.6187,  1.0071],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]),\n",
       " tensor([[[ 2.0276,  0.0108, -0.0283,  ...,  0.0319, -0.2828,  1.2022],\n",
       "          [ 0.7533, -0.1973, -0.9046,  ..., -0.6541, -0.5857, -0.2460],\n",
       "          [ 0.5321, -1.9123, -0.5533,  ...,  2.9644, -1.6402,  1.2700],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 1.2778, -0.6281, -1.3704,  ...,  1.1872,  1.5433,  1.2670],\n",
       "          [ 1.8381, -0.4869, -0.6455,  ...,  1.1009,  1.4978, -0.2726],\n",
       "          [ 0.9270,  0.1657, -0.6600,  ...,  1.3336,  1.2582,  1.7898],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.9974,  0.4707,  0.7235,  ...,  0.0945,  1.1234, -0.1385],\n",
       "          [ 3.5178,  1.7973, -2.4635,  ...,  1.8038,  0.5604,  0.8588],\n",
       "          [ 0.0796,  0.0507,  0.0663,  ..., -0.0403,  0.6273, -0.0086],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.1478, -0.7422, -2.9490,  ..., -1.0623,  1.9415, -1.0459],\n",
       "          [ 1.8381, -0.4869, -0.6455,  ...,  1.1009,  1.4978, -0.2726],\n",
       "          [ 0.9270,  0.1657, -0.6600,  ...,  1.3336,  1.2582,  1.7898],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 1.2778, -0.6281, -1.3704,  ...,  1.1872,  1.5433,  1.2670],\n",
       "          [ 1.8381, -0.4869, -0.6455,  ...,  1.1009,  1.4978, -0.2726],\n",
       "          [-0.0814,  0.0798,  0.9821,  ...,  1.2661, -2.3407,  0.8545],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 1.8381, -0.4869, -0.6455,  ...,  1.1009,  1.4978, -0.2726],\n",
       "          [-0.0632, -1.7298, -0.3497,  ...,  0.7920,  1.5458,  0.2662],\n",
       "          [ 0.9270,  0.1657, -0.6600,  ...,  1.3336,  1.2582,  1.7898],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27_10_23_01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('cc/2022-03-27_10_23_01/train', 'cc/2022-03-27_10_23_01/val')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "now = str(dt.now())\n",
    "time_path = now[:10] + \"_\" + now[11:13] + \"_\" + now[14:16] + \"_\" + now[17:19]\n",
    "print(time_path)\n",
    "tb_path = \"cc/\" + time_path\n",
    "tb_train_path = tb_path + \"/train\"\n",
    "tb_val_path = tb_path + \"/val\"\n",
    "tb_train_path, tb_val_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter \n",
    "writer_train = SummaryWriter(tb_train_path)\n",
    "writer_val = SummaryWriter(tb_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.003\n",
    "num_epoches = 100\n",
    "net = LSTM_seq2seq(100, 100).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "ctrl = optim.lr_scheduler.ExponentialLR(optimizer, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [22:04<00:00, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start training!\n",
    "for epoch in tqdm(range(num_epoches)):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        data, label = data\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "\n",
    "        if (output.shape[1] > label.shape[1]):\n",
    "            for k in range(output.shape[1] - label.shape[1]):\n",
    "                label = torch.cat((label, label[:,-1:,...]), dim=1)\n",
    "        elif (output.shape[1] < label.shape[1]):\n",
    "            for k in range(label.shape[1] - output.shape[1]):\n",
    "                output = torch.cat((output, output[:,-1:,...]), dim=1)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_avg_loss = train_loss / len(train_dataloader)\n",
    "    writer_train.add_scalar('Loss/Epoch', train_avg_loss, epoch+1) # epoch+1 because epoch starts from 0\n",
    "    writer_train.flush()\n",
    "    ctrl.step() # lr decay\n",
    "\n",
    "    net.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            data, label = data\n",
    "            output = net(data)\n",
    "            \n",
    "            if (output.shape[1] > label.shape[1]):\n",
    "                for k in range(output.shape[1] - label.shape[1]):\n",
    "                    label = torch.cat((label, label[:,-1:,...]), dim=1)\n",
    "            elif (output.shape[1] < label.shape[1]):\n",
    "                for k in range(label.shape[1] - output.shape[1]):\n",
    "                    output = torch.cat((output, output[:,-1:,...]), dim=1)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_avg_loss = val_loss / len(val_dataloader)\n",
    "    writer_val.add_scalar('Loss/Epoch', val_avg_loss, epoch+1) # epoch+1 because epoch starts from 0\n",
    "    writer_val.flush()\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save your language model...\n",
    "model_name = input(\"请输入语言模型的名称：\")\n",
    "model_name = model_name + '_' + time_path\n",
    "torch.save(net, \"nmt_model_paths/\" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"论语_2022-03-27_10_23_01\"\n",
    "net = torch.load(\"nmt_model_paths/\" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder_decoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下老回要也这地能于？"
     ]
    }
   ],
   "source": [
    "to_be_translated = input(\"Please input the sentence you want to translate: \")\n",
    "# to_be_translated = \"何谓也\"\n",
    "count = 10\n",
    "# input = [wv_classical[i] for i in input]\n",
    "# out = net.predict(to_be_translated, 20, wv_modern)\n",
    "lst = []\n",
    "for i in to_be_translated:\n",
    "    lst.append(wv_classical[i])\n",
    "input_ = torch.tensor(lst)\n",
    "# 升高一维\n",
    "input_ = input_.reshape((1, -1, 100))\n",
    "data, hidden = net.encoder(input_)\n",
    "for i in range(count):\n",
    "    y = net.decoder(data, hidden)[0][-1]\n",
    "    p = y.detach().numpy()\n",
    "    p = softmax(p)\n",
    "\n",
    "    idx = np.random.choice(np.arange(100), p=p)\n",
    "    # idx = torch.argmax(y, dim=0)\n",
    "    new_word = wv_modern.index_to_key[idx]\n",
    "    print(new_word, end='')\n",
    "    data = torch.cat((data, y.reshape((1, 1, 100))), dim=1)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
